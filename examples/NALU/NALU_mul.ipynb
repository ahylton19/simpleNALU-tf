{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Neural Arithmetic Logic Unit (NALU)\n",
    "Useful to learn the **multiplication/division** operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Neural Arithmetic Logic Unit\n",
    "def NALU(in_dim, out_dim):\n",
    "\n",
    "    shape = (int(in_dim.shape[-1]), out_dim)\n",
    "    epsilon = 1e-7 \n",
    "    \n",
    "    # NAC\n",
    "    W_hat = tf.Variable(tf.truncated_normal(shape, stddev=0.02))\n",
    "    M_hat = tf.Variable(tf.truncated_normal(shape, stddev=0.02))\n",
    "    G = tf.Variable(tf.truncated_normal(shape, stddev=0.02))\n",
    "        \n",
    "    W = tf.tanh(W_hat) * tf.sigmoid(M_hat)\n",
    "    # Forward propogation\n",
    "    a = tf.matmul(in_dim, W)\n",
    "    \n",
    "    # NALU  \n",
    "    m = tf.exp(tf.matmul(tf.log(tf.abs(in_dim) + epsilon), W))\n",
    "    g = tf.sigmoid(tf.matmul(in_dim, G))\n",
    "    y = g * a + (1 - g) * m\n",
    "    \n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Network by learning the addition\n",
    "\n",
    "# Generate a series of input number X1 and X2 for training\n",
    "x1 = np.arange(0,10000,5, dtype=np.float32)\n",
    "x2 = np.arange(5,10005,5, dtype=np.float32)\n",
    "\n",
    "\n",
    "y_train = x1 * x2\n",
    "\n",
    "x_train = np.column_stack((x1,x2))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# Generate a series of input number X1 and X2 for testing\n",
    "x1 = np.arange(1000,2000,8, dtype=np.float32)\n",
    "x2 = np.arange(1000,1500,4, dtype= np.float32)\n",
    "\n",
    "x_test = np.column_stack((x1,x2))\n",
    "y_test = x1 * x2\n",
    "\n",
    "print()\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, train, and test NALU network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the placeholder to feed the value at run time\n",
    "X = tf.placeholder(dtype=tf.float32, shape =[None , 2])    # Number of samples x Number of features (number of inputs to be added)\n",
    "Y = tf.placeholder(dtype=tf.float32, shape=[None,])\n",
    "\n",
    "# define the network\n",
    "# Here the network contains only one NAC cell (for testing)\n",
    "y_pred = NALU(in_dim=X, out_dim=1)\n",
    "y_pred = tf.squeeze(y_pred)             # Remove extra dimensions if any\n",
    "\n",
    "# Mean Square Error (MSE)\n",
    "loss = tf.reduce_mean( (y_pred - Y) **2)\n",
    "#loss= tf.losses.mean_squared_error(labels=y_train, predictions=y_pred)\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.05    # learning rate\n",
    "epochs = 2000\n",
    "\n",
    "optimize = tf.train.AdamOptimizer(learning_rate=alpha).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    #init = tf.global_variables_initializer()\n",
    "    cost_history = []\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # pre training evaluate\n",
    "    print(\"Pre training MSE: \", sess.run (loss, feed_dict={X: x_test, Y: y_test}))\n",
    "    print()\n",
    "    for i in range(epochs):\n",
    "        _, cost = sess.run([optimize, loss ], feed_dict={X: x_train, Y: y_train})\n",
    "        print(\"epoch: {}, MSE: {}\".format( i,cost) )\n",
    "        cost_history.append(cost)\n",
    "\n",
    "    # plot the MSE over each iteration\n",
    "    plt.plot(np.arange(epochs),np.log(cost_history))  # Plot MSE on log scale\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    # plt.savefig('nalu_DOT.png')\n",
    "    plt.show()\n",
    "\n",
    "  \n",
    "    # post training loss\n",
    "    print(\"Post training MSE: \", sess.run(loss, feed_dict={X: x_test, Y: y_test}))\n",
    "\n",
    "    print(\"Actual product: \", y_test[0:10])\n",
    "    print()\n",
    "    print(\"Predicted product: \", sess.run(y_pred[0:10], feed_dict={X: x_test, Y: y_test}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
